# Online Retail Data Pipeline â€” Case Study

## ğŸ“˜ Project Summary

This project implements a **complete end-to-end data engineering pipeline** for an online retail company, based on the *UCI Online Retail dataset (2010â€“2011)*.  
The goal is to deliver a **reliable, high-quality, and analytically optimized data pipeline** that ingests, cleans, validates, and models transaction data for reporting and insights.

The pipeline produces a **dimensional data model** designed for analytical queries, including:

- **Fact table:** `fact_sales` â€” combines sales and cancellations  
- **Dimension tables:** `dim_customers`, `dim_products`, `dim_time`  
- **Quarantine table:** `outliers` â€” stores anomalous records pending review

The design focuses on:

- âœ… **Data quality and validation** â€” automated integrity and completeness checks  
- âœ… **Maintainability** â€” config-driven file paths and thresholds  
- âœ… **Analytical readiness** â€” star schema warehouse design  
- âœ… **Transparency and reproducibility** â€” clear documentation and modular scripts


## ğŸ—‚ï¸ Project Structure

The repository is organized for clarity, modularity, and reproducibility.  

Below is the folder layout showing all key project components and their purpose:

```bash
foil_case_study/
â”œâ”€â”€ data/                           # Data assets generated and used by the pipeline
â”‚   â”œâ”€â”€ online_retail_raw.csv        # Raw dataset downloaded from UCI repository
â”‚   â”œâ”€â”€ transactions.csv             # Processed â€” cleaned sales transactions
â”‚   â”œâ”€â”€ cancellations.csv            # Processed â€” cleaned cancellation records
â”‚   â”œâ”€â”€ outliers.csv                 # Processed â€” records flagged as anomalies
â”‚   â””â”€â”€ warehouse/                   # Analytical layer (star schema)
â”‚       â”œâ”€â”€ fact_sales.parquet       # Fact table â€” sales and cancellations combined
â”‚       â””â”€â”€ outliers.parquet         # Quarantine table for anomalous records
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql                   # SQL schema defining fact and dimension tables
â”‚
â”œâ”€â”€ etl/                             # Core ETL pipeline scripts
â”‚   â”œâ”€â”€ extract.py                   # Data extraction from UCI repository
â”‚   â”œâ”€â”€ transform.py                 # Data cleaning, filtering, and outlier detection
â”‚   â”œâ”€â”€ validate.py                  # Automated data validation checks
â”‚   â””â”€â”€ load.py                      # Load step â€” builds warehouse tables in Parquet
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ helpers/
â”‚       â””â”€â”€ logger.py                # Centralized logging utility (future use)
â”‚
â”œâ”€â”€ tests/                           # Unit tests for ETL modules
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â””â”€â”€ test_transform.py
â”‚
â”œâ”€â”€ exploration/                     # Exploratory analysis notebooks
â”‚   â”œâ”€â”€ initial_exploration.ipynb
â”‚   â””â”€â”€ outliers_check.ipynb
â”‚
â”œâ”€â”€ config.yaml                      # Central configuration (paths, thresholds, parameters)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Excludes unnecessary files from version control
â””â”€â”€ README.md                        # Project documentation
```

## âš™ï¸ Setup & Instructions

### 1. Environment setup

Clone the repository and navigate to the project root:


```bash
git clone <your-repo-url>
cd foil_case_study
```
Create virtual environment (recommended)
```bash
python -m venv venv
```
Activate virtual environment
```bash
# macOS/Linux
source venv/bin/activate
```
```bash
# Windows
venv\Scripts\activate
```
Install dependencies
```bash
pip install -r requirements.txt
```

## ğŸ›  Configuration

The project uses a single `config.yaml` file to manage all file paths, thresholds, and parameters. This ensures the pipeline is maintainable and can run on different systems without code changes.

### Key points:

- **Paths**: Define locations for raw data, processed outputs (transactions, cancellations, outliers), and warehouse parquet files (fact and dimension tables).  
- **Outlier thresholds**: Specify quantity and unit price limits to flag anomalous records during transformation.

All ETL scripts (`extract.py`, `transform.py`, `validate.py`, `load.py`) read paths and thresholds from this file, keeping the pipeline fully configurable.



Instructions:
Place online_retail_raw.csv under /data/ before running scripts.


Design choice rationale:
At the transform stage, transactions and cancellations are stored separately to preserve their distinct business logic and simplify validation. In the warehouse layer, both are unified into a single fact table (fact_sales) with an indicator column (transaction_type) to support net-sales analytical queries.

# Dimensional Data Model

To support analytical queries on retail transactions, we designed a dimensional data model consisting of a fact table and multiple dimension tables.

## Fact Table: `fact_sales`

**Purpose:** Stores individual sales transactions, including both completed and cancelled orders.

**Columns:**

- `transaction_key` â€“ surrogate primary key (`SERIAL`)
- `invoice_no` â€“ original invoice number (`VARCHAR(10) NOT NULL`)
- `customer_key` â€“ foreign key to `dim_customers`
- `product_key` â€“ foreign key to `dim_products`
- `date_key` â€“ foreign key to `dim_time` (date of transaction)
- `transaction_datetime` â€“ exact timestamp of the transaction (`TIMESTAMP NOT NULL`)
- `quantity` â€“ number of units sold (`INT NOT NULL`)
- `unit_price` â€“ price per unit (`DECIMAL(10,2) NOT NULL`)
- `sign` â€“ `+1` for sales, `-1` for cancellations (`SMALLINT`)
- `total_amount` â€“ calculated as `quantity * unit_price * sign` (`DECIMAL(12,2) GENERATED ALWAYS AS ... STORED`)
- `transaction_type` â€“ `"SALE"` or `"CANCEL"` (`VARCHAR(10)`)

**Rationale:**  
Using a surrogate `transaction_key` simplifies joins and ensures uniqueness even if `invoice_no` is not strictly unique. Including `sign` and `transaction_type` allows straightforward revenue calculations and proper handling of cancellations. Storing `transaction_datetime` preserves the exact time of each transaction for time-based analyses. The `total_amount` is calculated at the database level to avoid repeated computations in queries.

---

## Dimension Table: `dim_customers`

**Purpose:** Stores customer information for analysis and segmentation.

**Columns:**

- `customer_key` â€“ surrogate primary key (`SERIAL`)
- `customer_id` â€“ original 5-digit customer identifier (`VARCHAR(5) NOT NULL`)
- `country` â€“ customer's country (`VARCHAR(50)`)

**Rationale:**  
A surrogate key ensures consistency in joins with the fact table. Customer attributes are stored in a separate table to reduce redundancy and support easy filtering by customer properties.

---
## Dimension Table: `dim_products`

**Purpose:** Stores product details to analyze sales by item.

**Columns:**

- `product_key` â€“ surrogate primary key (`SERIAL`)
- `stock_code` â€“ original product code (`VARCHAR(5) NOT NULL`)
- `description` â€“ product name (`VARCHAR(255)`)

**Rationale:**  
Separating product information into a dimension table avoids repeated storage of product names in the fact table and simplifies product-level analyses. Surrogate keys allow consistent joins and prevent duplication issues.
---

## Dimension Table: `dim_time`

**Purpose:** Stores date-level information for time-based analytics.

**Columns:**

- `date_key` â€“ date of transaction (`DATE PRIMARY KEY`)
- `year` â€“ year of the transaction (`INT`)
- `month` â€“ month number (`INT`)
- `day` â€“ day of month (`INT`)
- `weekday` â€“ day of week (`VARCHAR(10)`)

**Rationale:**  
A separate time dimension allows flexible grouping, filtering, and aggregation by year, quarter, month, or weekday. This design improves query performance and simplifies date-based calculations in analytical reports.

---

## Outliers Table: `outliers`

**Purpose:** Temporary quarantine for unusual transactions to be reviewed manually.

**Columns:**
- All columns match `fact_sales` (`invoice_no`, `customer_key`, `product_key`, `date_key`, `transaction_datetime`, `quantity`, `unit_price`, `sign`, `total_amount`, `transaction_type`)

**Rationale:**  
Keeping the same structure as `fact_sales` allows easy reinsertion of reviewed outliers back into the fact table. No separate dimension tables are needed.

**Notes:**
- The outliers table is **outside the main star schema**, serving as a quarantine.  
- Once outliers are reviewed, they can be merged into `fact_sales` without needing transformations or separate dimension tables.  
- This design keeps the warehouse clean, normalized, and ready for analytical queries.

**Overall Rationale:**  
This dimensional model follows the star schema pattern, with a central fact table (`fact_sales`) connected to dimension tables (`dim_customers`, `dim_products`, `dim_time`). It supports efficient analytical queries, reduces redundancy, and provides flexibility for aggregation and slicing of data along different dimensions. Surrogate keys simplify joins and ensure uniqueness across the schema.

Rationale: Simplifies reporting by precomputing useful time attributes, avoids repeated extraction logic from InvoiceDate.

Design Principles

Star Schema: Fact table at the center, dimension tables surrounding it.

Normalization: Dimension tables remove redundancy from the fact table.

Analytical efficiency: Queries like total revenue per customer or sales by product and day are simplified.

Flexibility: Cancellations and outliers are handled via transaction_type and separate tables, preserving data integrity.